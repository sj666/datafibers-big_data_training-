## The data comes originally from RITA where it is described in detail. You can download the data there, or from the bzipped csv files listed below. These files have derivable variables removed, are packaged in yearly chunks and have been more heavily compressed than the originals.

# ref : http://stat-computing.org/dataexpo/2009/the-data.html

# Data description
  Name	          Description
1	Year	          1987-2008
2	Month	          1-12
3	DayofMonth	    1-31
4	DayOfWeek	      1 (Monday) - 7 (Sunday)
5	DepTime	        actual departure time (local, hhmm)
6	CRSDepTime	    scheduled departure time (local, hhmm)
7	ArrTime	        actual arrival time (local, hhmm)
8	CRSArrTime	    scheduled arrival time (local, hhmm)
9	UniqueCarrier	  unique carrier code
10	FlightNum	      flight number
11	TailNum	        plane tail number
12	ActualElapsedTime	  in minutes
13	CRSElapsedTime	  in minutes
14	AirTime	      in minutes
15	ArrDelay	    arrival delay, in minutes
16	DepDelay	    departure delay, in minutes
17	Origin	      origin IATA airport code
18	Dest	        destination IATA airport code
19	Distance	    in miles
20	TaxiIn	      taxi in time, in minutes
21	TaxiOut	      taxi out time in minutes
22	Cancelled	    was the flight cancelled?
23	CancellationCode	  reason for cancellation (A = carrier, B = weather, C = NAS, D = security)
24	Diverted	    1 = yes, 0 = no
25	CarrierDelay	    in minutes
26	WeatherDelay	    in minutes
27	NASDelay	    in minutes
28	SecurityDelay	    in minutes
29	LateAircraftDelay	    in minutes

0. File content preview
cd RITA_Data
head 2000/2000.csv

1. Upload the file to HDFS
hadoop fs -put RITA_Data /stage-data/.

2. Verify the upload
hadoop fs -ls -R /stage-data/RITA_Data

3. Hive operations
----------------------------------------------------------------------------------------------------------
beeline 
!connect jdbc:hive2://localhost:10000/default

use lecture;
CREATE DATABASE lecture;
CREATE EXTERNAL TABLE `lecture.flight_data_tmp`(
   `year` int,
   `month` int,
   `day` int,       
   `day_of_week` int,
   `dep_time` int,
   `crs_dep_time` int,
   `arr_time` int,
   `crs_arr_time` int,
   `unique_carrier` string,
   `flight_num` int,
   `tail_num` string,
   `actual_elapsed_time` int,
   `crs_elapsed_time` int,
   `air_time` int,
   `arr_delay` int,
   `dep_delay` int,
   `origin` string,
   `dest` string,
   `distance` int,
   `taxi_in` int,
   `taxi_out` int,
   `cancelled` int,
   `cancellation_code` string,
   `diverted` int,
   `carrier_delay` string,
   `weather_delay` string,
   `nas_delay` string,
   `security_delay` string,
   `late_aircraft_delay` string)
 PARTITIONED BY (`etl_year` string)
 ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ','
 STORED AS INPUTFORMAT
   'org.apache.hadoop.mapred.TextInputFormat'
 OUTPUTFORMAT
   'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
 LOCATION
   'hdfs://sandbox-hdp.hortonworks.com:8020/user/hive/warehouse/lecture.db/flight_data_tmp';
   
CREATE EXTERNAL TABLE `lecture.flight_data_orc`(
   `month` int,
   `day` int,       
   `day_of_week` int,
   `dep_time` int,
   `crs_dep_time` int,
   `arr_time` int,
   `crs_arr_time` int,
   `unique_carrier` string,
   `flight_num` int,
   `tail_num` string,
   `actual_elapsed_time` int,
   `crs_elapsed_time` int,
   `air_time` int,
   `arr_delay` int,
   `dep_delay` int,
   `origin` string,
   `dest` string,
   `distance` int,
   `taxi_in` int,
   `taxi_out` int,
   `cancelled` int,
   `cancellation_code` string,
   `diverted` int,
   `carrier_delay` string,
   `weather_delay` string,
   `nas_delay` string,
   `security_delay` string,
   `late_aircraft_delay` string)
 PARTITIONED BY (`year` string)
 STORED AS ORC
 LOCATION
   'hdfs://sandbox-hdp.hortonworks.com:8020/user/hive/warehouse/lecture.db/flight_data_orc';
-------------------------------------------------------------------------------------------------------

########################################################################################################
File : /user/oozie/workflow/lecture_02/workflow.xml
--------------------------------------------------------------------------------------------------------
<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<workflow-app name="lecture_01" xmlns="uri:oozie:workflow:0.5" xmlns:sla="uri:oozie:sla:0.2">
   <global/>
   <start to="hive_action_1"/>
   <kill name="Kill">
      <message>Action Failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
   </kill>
   <action name="hive_action_1">
       <hive2 xmlns="uri:oozie:hive2-action:0.2">
           <job-tracker>${jobTracker}</job-tracker>
           <name-node>${nameNode}</name-node>
           <prepare/>
           <configuration>
              <property>
                  <name>mapred.job.queue.name</name>
                  <value>${queueName}</value>
              </property>
           </configuration>
           <jdbc-url>jdbc:hive2://localhost:10000/default</jdbc-url>
           <password>hive</password>
           <script>lib/load_csvfile.hql</script>
           <param>ETL_YEAR=${YEAR}</param>
       </hive2>
       <ok to="hive_action_2"/>
       <error to="Kill"/>
   </action>
   <action name="hive_action_2">
       <hive2 xmlns="uri:oozie:hive2-action:0.2">
           <job-tracker>${jobTracker}</job-tracker>
           <name-node>${nameNode}</name-node>
           <prepare/>
           <configuration>
              <property>
                  <name>mapred.job.queue.name</name>
                  <value>${queueName}</value>
              </property>
           </configuration>
           <jdbc-url>jdbc:hive2://localhost:10000/default</jdbc-url>
           <password>hive</password>
           <script>lib/copy_to_orc.hql</script>
           <param>YEAR=${YEAR}</param>
       </hive2>
       <ok to="end"/>
       <error to="Kill"/>
    </action>

   <end name="end"/>
</workflow-app>

-------------------------------------------------------------------------------------------------

#################################################################################################
File : /user/oozie/workflow/lecture_02/lib/load_csvfile.hql
----------------------------------------------------------------------------------------------------------------------------
LOAD DATA INPATH '/stage-data/RITA_Data/${ETL_YEAR}/*.csv' INTO TABLE lecture.flight_data_tmp PARTITION(etl_year=${ETL_YEAR});
----------------------------------------------------------------------------------------------------------------------------

#################################################################################################
File : /user/oozie/workflow/lecture_02/lib/copy_to_orc.hql
-------------------------------------------------------------------------------------------------
INSERT OVERWRITE TABLE lecture.flight_data_orc 
PARTITION (year='${ETL_YEAR}')
SELECT
   month,
   day,
   day_of_week,
   dep_time,
   crs_dep_time,
   arr_time,
   crs_arr_time,
   unique_carrier,
   flight_num,
   tail_num,
   actual_elapsed_time,
   crs_elapsed_time,
   air_time,
   arr_delay,
   dep_delay,
   origin,
   dest,
   distance,
   taxi_in,
   taxi_out,
   cancelled, 
   cancellation_code, 
   diverted,
   carrier_delay,
   weather_delay,
   nas_delay,
   security_delay,
   late_aircraft_delay
FROM lecture.flight_data_tmp WHERE etl_year = '${YEAR}';

####################################################################################################
File : /user/oozie/workflow/lecture_02/job.properties
----------------------------------------------------------------------------------------------------
#
#Tue Apr 24 10:31:14 KST 2018
user.name=mapred
oozie.use.system.libpath=true
oozie.wf.application.path=${nameNode}/user/oozie/workflow/lecture_02
queueName=default
nameNode=hdfs://sandbox-hdp.hortonworks.com:8020
oozie.libpath=
jobTracker=sandbox-hdp.hortonworks.com\:8032
YEAR=2000
-----------------------------------------------------------------------------------------------------

### oozie job prepare
1. workflow upload to HDFS
hadoop fs -put (-f) lecture_02 /user/oozie/workflow/.

2. job.properties.
cd lecture_02

3. oozie CLI command to run the job
oozie job -config job.properties -run

4. oozie workflow job infomation check
oozie job -info {job_id}

5. hive_action_2 information check
oozie job -info {job_id}@hive_action_2

6. STDOUT/STDERR from yarn
http://sandbox-hdp.hortonworks.com:8088/proxy/{yarn application id}/

7. hql operations
 * copy_to_orc.hql to ${ETL_YEAR} and ${YEAR}
hadoop fs -put -f lib /user/oozie/workflow/lecture_02

8. oozie job rerun
oozie job -rerun {job id} -Doozie.wf.rerun.failnodes=true

9. stage-data file verify
hadoop fs -ls -R /stage-data/RITA_Data/

10. flight_data_tmp table verify data
hadoop fs -ls -R /user/hive/warehouse/lecture.db/flight_data_tmp

11. hive operations
[root@sandbox-hdp lecture_02]# beeline -u jdbc:hive2://sandbox-hdp.hortonworks.com:10000/lecture -n hive -p hive
0: jdbc:hive2://sandbox-hdp.hortonworks.com:1> SELECT count(*) FROM lecture.flight_data_tmp WHERE etl_year=2000;
0: jdbc:hive2://sandbox-hdp.hortonworks.com:1> SELECT count(*) FROM lecture.flight_data_orc WHERE year=2000;
